[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this guide",
    "section": "",
    "text": "This tutorial provides you with a comprehensive introduction to Small Area Estimation (SAE) methods using R. It is designed to help geographers, urban analysts, and geospatial data scientists understand and apply the principles of SAE in their work. The tutoral focuses on regression-based SAE methods, and covers the following key topics:\n\nIntroduction to SAE: Understanding the need for SAE in geography and urban analysis.\nDirect Estimators: Exploring the limitations of direct estimators in small area contexts.\nRegression-Based Methods: Introduction to the area-level and unit-level models.\nPractical Applications: Implementing SAE methods in R using real-world datasets.\n\nThe tutorial is designed to be accessible to those with a basic understanding of statistical modelling. A familiarity with R is beneficial, but not strictly necessary, as the tutorial includes already prepared code snippets for practical implementation.\nAll the materials are available through the Workshop’s GitHub repository, available here:",
    "crumbs": [
      "About this guide"
    ]
  },
  {
    "objectID": "introduction/sub2.html",
    "href": "introduction/sub2.html",
    "title": "Subsection Two",
    "section": "",
    "text": "Content for the second subsection page.\n\n\nAdditional notes.",
    "crumbs": [
      "Introduction",
      "Subsection Two"
    ]
  },
  {
    "objectID": "introduction/sub2.html#notes",
    "href": "introduction/sub2.html#notes",
    "title": "Subsection Two",
    "section": "",
    "text": "Additional notes.",
    "crumbs": [
      "Introduction",
      "Subsection Two"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Small Area Estimation",
    "section": "",
    "text": "1 What is Small Area Estimation?\nSmall area estimation (SAE) is a statistical methodology that aims to produce reliable estimates at high spatial resolutions. Surveys are designed to produce reliable estimates for larger geographic areas, such as countries or regions. However, researchers, analysts, and policymakers often face challenges that require estimates for smaller geographic areas, such as districts or neighbourhoods. Producing estimates for these smaller areas using survey data designed for larger areas can be problematic, as the sample sizes are often too small or even non-existent for those smaller areas. Small area estimation methods address this issue by implementing methodologies that allow for the production of reliable estimates in these cases.\n\n\n2 SAE applications\nSAE methods have been widely applied in mulitple fields. These methods were originally developed to address the need for reliable data to tackle issues such as poverty, health, and education at local levels. They are particularly useful in contexts where access to official statistics is limited or the available data is not updated frequently enough to inform timely policy decisions. These methods are commonly used by official statistical offices and institutions, such as the Office for National Statistics (ONS) in the UK and the World Bank (Corral et al., 2022; Office for National Statistics, 2023).\nSmall area estimation methods aim at producing reliable estimates for geographic areas or subpopulation groups —from now on, domains— for which survey samples are scarce or even non-existent. Easy fixes for this problems could involve increasing the sample sizes, or changing the sampling design so that domains that were initially unplanned, become planned. This, however, is in most cases not possible because of monetary constraints or because the survey already exists. Therefore, indirect methods are required. These more sophisticated methods take advantage of the existing relationships in the data, relying on statistical models to describe these relationships and filling in the gaps where data is scarce. These auxiliary variables “borrow strength” from cross-sectional data or time and/or spatial correlation to produce more reliable estimates.\nSmall area estimation (SAE) has been widely applied across sectors to produce detailed, policy-relevant statistics that would otherwise be unavailable or unreliable. A prominent example is in the field of poverty alleviation, where SAE methods have had tangible impacts. Policymakers aiming to reduce poverty often face the dual challenge of limited budgets and a need for granular, local-level information to effectively target interventions. The demand for cost-effective, disaggregated data has driven the adoption of indirect estimation techniques, particularly for producing poverty indicators at the subnational level. In response, organisations such as the World Bank have employed SAE methods to generate high-resolution poverty maps that improve our understanding of the spatial distribution of poverty and help guide resource allocation (Corral et al., 2022)\nBeyond poverty estimation, SAE has been key in filling data gaps in other domains. In the UK, for example, the national census has never included questions on income, despite income being a crucial variable for policymakers, researchers, and other data users. To address this, the Office for National Statistics (ONS) uses SAE to estimate income levels in areas where survey data alone are insufficient (OfficeforNationalStatistics2016?). Through a method known as synthetic estimation, the ONS combines household survey responses with auxiliary data—such as administrative records and census-derived indicators—that are available for all geographic units in the target population. By modelling the relationship between individual income and area-level covariates (e.g., the proportion of residents claiming Income Support), the ONS fits a regression model using sampled areas and applies it nationally under the assumption that the relationships hold across all areas. This enables the production of robust and consistent income estimates for small geographic units, such as Middle-layer Super Output Areas (MSOAs), even in the absence of direct survey data.\n\n\n3 Why do we need it?\nBrief overview text. You can add images, code, etc.\n\n\n\n\n\nReferences\n\nCorral, P., Molina, I., Cojocaru, A., & Segovia, S. (2022). Guidelines to small area estimation for poverty mapping. 2022.\n\n\nOffice for National Statistics. (2023). Small area income estimates for middle layer super output areas, england and wales [Dataset]. Office for National Statistics. https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/smallareaincomeestimatesformiddlelayersuperoutputareasenglandandwales",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/sub1.html",
    "href": "introduction/sub1.html",
    "title": "Subsection One",
    "section": "",
    "text": "Content for the first subsection page.\n\n\nMore detail here.",
    "crumbs": [
      "Introduction",
      "Subsection One"
    ]
  },
  {
    "objectID": "introduction/sub1.html#details",
    "href": "introduction/sub1.html#details",
    "title": "Subsection One",
    "section": "",
    "text": "More detail here.",
    "crumbs": [
      "Introduction",
      "Subsection One"
    ]
  },
  {
    "objectID": "workshop/practical/practical.html",
    "href": "workshop/practical/practical.html",
    "title": "A practical guide on Small Area Estimation",
    "section": "",
    "text": "Please, make sure you have R version 4.5.1 (2025-06-13) installed in your laptop. You can download it from here: https://cran.r-project.org/\n# Install required packages if not already installed\nrequired_packages &lt;- c(\n  \"sae\", \"emdi\", \"saeTrafo\", \"hbsae\", \"SUMMER\", \"survey\",\n  \"dplyr\", \"tidyr\", \"purrr\", \"sf\", \n  \"ggplot2\", \"hrbrthemes\", \"GGally\", \"patchwork\", \"plotly\"\n)\n\nto_install &lt;- setdiff(required_packages, rownames(installed.packages()))\nif (length(to_install) &gt; 0) {\n  install.packages(to_install)\n}\n\n# Install INLA for Bayesian SAE estimators\nif (!isTRUE(requireNamespace(\"INLA\", quietly = TRUE))) {\n  install.packages(\"INLA\", repos=c(getOption(\"repos\"), \n                  INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE)\n}\n\nlibrary(sae)          # Small area estimation models\nlibrary(emdi)         # Example datasets\nlibrary(saeTrafo)     # For domain sizes\nlibrary(SUMMER)       # Bayesian SAE\nlibrary(survey)       # Survey designs\n\nlibrary(dplyr)        # For data wrangling\nlibrary(tidyr)        # For data wrangling\nlibrary(purrr)        # For data wrangling\nlibrary(sf)           # For mapping\nlibrary(ggplot2)      # For visualisations\nlibrary(hrbrthemes)   # For visualisations\nlibrary(GGally)       # For visualisations\nlibrary(patchwork)    # For visualisations\nlibrary(viridis)      # For visualisations\nlibrary(cowplot)      # For visualisations\nlibrary(plotly)       # For interactive visualisations"
  },
  {
    "objectID": "workshop/practical/practical.html#conclusion",
    "href": "workshop/practical/practical.html#conclusion",
    "title": "Mapping the Unseen: Small Area Estimation for Urban Analysis",
    "section": "3.1 Conclusion",
    "text": "3.1 Conclusion\nWe have seen how the direct estimate can produce estimates with minimal information. However, these results can be unreliable when sample sizes are small, and they cannot even be produced if the domain of interest is not included in the sample. To solve this problem, we rely on model-based small area estimation methods."
  },
  {
    "objectID": "workshop/practical/practical.html#small-area-estimation-in-r",
    "href": "workshop/practical/practical.html#small-area-estimation-in-r",
    "title": "Mapping the Unseen: Small Area Estimation for Urban Analysis",
    "section": "4.1 Small Area Estimation in R",
    "text": "4.1 Small Area Estimation in R\nBefore we start calculating our estimates, we will learn a bit more about the available R packages for small area estimation. In this practical we will be using two of the most commonly used packages for small area estimation modelling: sae and SUMMER. Although both packages aim at producing small area estimates in the presence of small sample sizes, they are based on two fundamentally different paradigms.\nThe sae package is based on frequentist methods, while SUMMER adopts a Bayesian approach to producing estimates. While in the frequentist approach the model estimates are assumed to be fixed, Bayesian models treat parameters as random variables. The Bayesian framework allows for incorporating prior information that allows for more flexible modelling, particularly in the presence of scarce or highly variable data, as is the case of small area estimation."
  },
  {
    "objectID": "workshop/practical/practical.html#area-level-model",
    "href": "workshop/practical/practical.html#area-level-model",
    "title": "Mapping the Unseen: Small Area Estimation for Urban Analysis",
    "section": "4.2 Area-level model",
    "text": "4.2 Area-level model\nArea-level estimates use domain-level data to improve the performance of direct estimates. The basic area-level model, the Fay-Herriot model, uses a two-step approach to do this: first, the model estimates a direct estimator in what is known as the sampling model. Next, the model computes area-specific random effects in what is known as the linking model.\nThe sampling model is formalised as follows:\n\\[\n\\hat{\\theta}_i^{\\mathrm{DIR}} = \\theta_i + \\epsilon_i; \\quad \\epsilon_i \\sim^{\\mathrm{ind}} \\mathcal{N}(0, V_i), \\quad i = 1, \\ldots, M\n\\]\nWhere \\(V_i\\) is the sampling variance of the direct estimator \\(\\hat{\\theta}_i^{\\mathrm{DIR}}\\) and \\(\\epsilon_i\\) represents the sampling error, which is assumed to be independently distributed.\nThe linking model allows us to borrow strength from other areas by assuming the small area parameter \\(\\theta_i\\) is related to auxiliary variables \\(x_i = (x_{i1}, x_{i2}, ..., x_{ip})'\\) through a linear regression model given by:\n\\[\n\\theta_i = x_i' \\beta + u_i = \\alpha + \\beta \\bar{x_i}; \\quad u_i \\sim^{\\mathrm{ind}} \\mathcal{N}(0, \\sigma_u^2), \\quad i = 1, \\ldots, M\n\\]\nwhere \\(\\beta = (\\beta_1, \\beta_2..., \\beta_p)'\\) is a vector with the regression coefficients, and \\(u_i\\) are area-specific random effects, also assumed to be independent and identically distributed (IID).\nAlthough the basic Fay-Herriot model assumes the area-specific effects are independent, and identically distributed, we observe that this assumption, many times, does not hold. Often, we see that the values of our target variable in one domain are significantly correlated to those of other areas nearby. The error term in our model then becomes:\n\\[\nu = \\rho_1 Wu + \\epsilon; \\quad \\epsilon \\sim \\mathcal{N}(0_i, \\sigma_I^2 I_i)\n\\] where \\(I_i\\) is the identity matrix for the domains, and \\(0_i\\) is a vector of zeros of the size of the total domains. Additionally, \\(\\rho_1 \\in (-1,1)\\) is an autoregression parameter and \\(W\\) is and adjacency matrix.\n\n4.2.1 Intercept-only model\nThe most basic area-level model is the intercept-only model. This model estimates a common mean for all estimates, but allows each of them to deviate from it based on an area-level random effect. The Fay-Herriot model “shrinks” those unreliable direct estimates –those with high variance– towards the global mean, producing more reliable estimates that borrow strength from other areas.\nWe model the direct survey estimate \\(\\hat{Y}_i\\) for each area \\(i\\) as:\n\\[\n\\hat{Y}_i = \\beta_0 + u_i + e_i\n\\]\nWhere:\n- \\(\\hat{Y}_i\\) is the direct survey estimate for area \\(i\\)\n- \\(\\beta_0\\) is the overall intercept (global mean)\n- \\(u_i \\sim N(0, \\sigma_u^2)\\) area-specific random effect\n- \\(e_i \\sim N(0, D_i)\\) is the sampling error, with known variance \\(D_i\\) from the survey.\nThis linear mixed model includes the fixed term \\(\\beta_0\\) which estimates the common mean, and the random part \\(u_i\\) and \\(e_i\\) that accounts for the area specific variation, and the variation due to the sampling error.\nIn SUMMER, the area-level model is calculated using the smoothArea() function.\n\n# Total population\ndomsize &lt;- eusilcA_pop %&gt;%\n  mutate(district = as.character(district)) %&gt;%\n  group_by(district) %&gt;%\n  count(name = \"size\") %&gt;%\n  arrange(district)\n\n# Scale the variable\neusilcA_smp2 &lt;- eusilcA_smp\nsd1 &lt;- sd(eusilcA_smp2$eqIncome)\neusilcA_smp2$eqIncome &lt;- eusilcA_smp2$eqIncome / sd1\n\n# Produce direct estimate with re-scaled data\ndir_est &lt;- sae::direct(\n  y = eusilcA_smp2$eqIncome,\n  dom = eusilcA_smp2$district,\n  sweight = eusilcA_smp2$weight,\n  domsize = smp_area_size_df,\n  replace = FALSE\n) %&gt;% \n  mutate(Var = SD^2) %&gt;% \n  dplyr::select(Domain, Direct, Var)\n\n# Produce covariate matrix and scale the values\n# Define covariates\ncovariates &lt;- c(\"cash\", \"unempl_ben\", \"tax_adj\")\n\n# Extract and scale covariates from eusilcA_popAgg\nXmat_scaled &lt;- eusilcA_popAgg %&gt;%\n  dplyr::select(district = Domain, all_of(covariates)) %&gt;%\n  mutate(across(all_of(covariates), ~ . / sd(.)))\n\n# Fit area-level intercept-only model\nfit &lt;- smoothArea(\n  formula = eqIncome ~ 1, # Intercept only\n  domain = ~district,     # Domain of interest \n  direct.est = dir_est,   # Pre-computed direct estimator\n  X.domain = domsize      # Add a matrix with all (even out-of-sample) domains\n)\n\nThere are domains in X.domain not in design/direct estimates. \nGenerating estimates for all domains in X.domain.\n\n\nWarning in smoothArea(formula = eqIncome ~ 1, domain = ~district, direct.est =\ndir_est, :\n\n# Format results (un-scale them)\narea_level_intercept &lt;- fit$iid.model.est %&gt;%\n  mutate(\n    mean   = mean * sd1,\n    median = median * sd1,\n    lower  = lower * sd1,\n    upper  = upper * sd1,\n    var    = var * sd1^2,\n    se     = sqrt(var)\n  )\n\n# See results\nhead(area_level_intercept)\n\n               domain     mean   median      var    lower    upper\n1           Amstetten 16167.62 16058.34  4612589 11950.77 20667.92\n2               Baden 20018.60 19910.04  5805194 15619.08 24333.68\n3             Bludenz 14595.30 14873.22  5274993 10238.49 19078.34\n4      Braunau am Inn 13823.45 14059.04  3668061 10078.00 17421.74\n5             Bregenz 21964.56 21707.20 10583253 16120.99 28829.20\n6 Bruck an der Leitha 19566.41 19316.03  8202519 14502.14 25719.52\n                 method       se\n1 Area level model: IID 2147.694\n2 Area level model: IID 2409.397\n3 Area level model: IID 2296.735\n4 Area level model: IID 1915.218\n5 Area level model: IID 3253.191\n6 Area level model: IID 2864.004\n\n\n\n\n4.2.2 Fay-Herriot model with auxiliary information\nWe can estimate a slightly more complex model by adding area-level covariates. The Fay-Herriot model will combine the direct estimates with area-level auxiliary data, in this case, the variable cash. The model is estimated as follows:\n\\[\n\\hat{Y}_i = \\beta_0 + \\beta_1 \\cdot \\text{cash}_i + \\beta_2 \\cdot \\text{unempl_ben} + \\beta_3 \\cdot \\text{tax_adj} + u_i + e_i\n\\]\n\n# Produce covariate matrix and scale the values\n# Define covariates\ncovariates &lt;- c(\"cash\", \"unempl_ben\", \"tax_adj\")\n\n# Extract and scale covariates from eusilcA_popAgg\nXmat_scaled &lt;- eusilcA_popAgg %&gt;%\n  dplyr::select(district = Domain, all_of(covariates)) %&gt;%\n  mutate(across(all_of(covariates), ~ . / sd(.)))\n\n# Area-level estimate with covariates\nfit &lt;- smoothArea(\n  formula = eqIncome ~ cash + unempl_ben + tax_adj,\n  domain = ~district,\n  direct.est = dir_est,\n  return.samples = T,\n  X.domain = Xmat_scaled\n)\n\nThere are domains in X.domain not in design/direct estimates. \nGenerating estimates for all domains in X.domain.\n\n\nWarning in smoothArea(formula = eqIncome ~ cash + unempl_ben + tax_adj, :\n\n# Un-scale my outputs\narea_level_covariates &lt;- fit$iid.model.est %&gt;%\n  mutate(\n    mean   = mean * sd1,\n    median = median * sd1,\n    lower  = lower * sd1,\n    upper  = upper * sd1,\n    var    = var * sd1^2,\n    se     = sqrt(var)\n  )\n\n# See outputs\nhead(area_level_covariates)\n\n               domain     mean   median       var    lower    upper\n1           Amstetten 12990.28 12962.08  864613.4 11345.78 14997.11\n2               Baden 20225.95 20234.31 1262233.6 17949.63 22304.98\n3             Bludenz 12505.37 12650.00 1412789.5 10184.41 14881.25\n4      Braunau am Inn 12121.37 12189.38  649620.1 10384.85 13519.82\n5             Bregenz 30894.49 30708.21 3551527.3 27468.52 34562.24\n6 Bruck an der Leitha 22308.97 22281.82 1682006.6 19928.98 24814.23\n                 method        se\n1 Area level model: IID  929.8459\n2 Area level model: IID 1123.4917\n3 Area level model: IID 1188.6082\n4 Area level model: IID  805.9901\n5 Area level model: IID 1884.5496\n6 Area level model: IID 1296.9220\n\n\n\n\n4.2.3 Spatial model\nThe last iteration for the area-level model involves incorporating the spatial component. This extension of the Fay-Herriot model assumes spatial correlation between areas, assuming that the values of one area will be influenced by the values of the areas around it.\n\\[\n\\boldsymbol{u} = \\rho W \\boldsymbol{u} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I)\n\\]\n\n# Rename the proximity matrix's names and rows\ncolnames(eusilcA_prox) &lt;- rownames(eusilcA_prox) &lt;- eusilcA_popAgg$Domain\n\n# Area-level model with spatial effects estimate\nfit &lt;- smoothArea(\n  formula = eqIncome ~ cash + unempl_ben + tax_adj,\n  domain = ~district,\n  direct.est = dir_est,\n  return.samples = T,\n  X.domain = Xmat_scaled,\n  adj.mat = eusilcA_prox   # Add adjacency matrix\n)\n\nThere are domains in X.domain not in design/direct estimates. \nGenerating estimates for all domains in X.domain.\n\n\nWarning in smoothArea(formula = eqIncome ~ cash + unempl_ben + tax_adj, :\n\n# Un-scale my outputs\narea_level_spatial &lt;- fit$bym2.model.est %&gt;%\n  mutate(\n    mean   = mean * sd1,\n    median = median * sd1,\n    lower  = lower * sd1,\n    upper  = upper * sd1,\n    var    = var * sd1^2,\n    se     = sqrt(var)  \n  )\n\nhead(area_level_spatial)\n\n               domain     mean   median       var    lower    upper\n1           Amstetten 12989.15 12991.85  854783.5 11227.32 14688.10\n2               Baden 20219.41 20230.40 1224878.4 18135.83 22336.97\n3             Bludenz 12469.61 12512.38 1717654.9 10111.62 15188.55\n4      Braunau am Inn 12219.91 12316.25  610878.8 10734.87 13651.76\n5             Bregenz 30621.18 30596.72 3345092.2 27466.12 34070.74\n6 Bruck an der Leitha 22334.29 22357.39 1343223.7 20010.24 24478.22\n                  method        se\n1 Area level model: BYM2  924.5450\n2 Area level model: BYM2 1106.7422\n3 Area level model: BYM2 1310.5933\n4 Area level model: BYM2  781.5873\n5 Area level model: BYM2 1828.9593\n6 Area level model: BYM2 1158.9753\n\n\n\n\n4.2.4 Model comparison\nNow that all the area-level models have been estimated, we can compare the results. For all three models, we observe high correlation of the estimates\n\n# Prepare data for model comparison\narea_level_summer_est &lt;- data.frame(\n  Domain = area_level_intercept$domain,\n  Intercept_model_est = area_level_intercept$mean,\n  Covariates_model_est = area_level_covariates$mean,\n  Spatial_model_est = area_level_spatial$mean)\n\n\narea_level_summer_se &lt;- data.frame(\n  Domain = area_level_intercept$domain,\n  Intercept_model_se = area_level_intercept$se,\n  Covariates_model_se = area_level_covariates$se,\n  Spatial_model_se = area_level_spatial$se\n)\n\n# Flag out-of-sample domains\noos &lt;- setdiff(eusilcA_pop$district,eusilcA_smp$district)\narea_level_summer_est &lt;- area_level_summer_est %&gt;%\n  mutate(sample_status = if_else(Domain %in% oos, \"out-of-sample\", \"in-sample\"))\n\narea_level_summer_se &lt;- area_level_summer_se %&gt;%\n  mutate(sample_status = if_else(Domain %in% oos, \"out-of-sample\", \"in-sample\"))\n\n# Plot\nggpairs(\n  data = area_level_summer_est[,-1],\n  aes(color = sample_status)\n) +\n  theme_ipsum()\n\n\n\n\n\n\n\nggpairs(\n  data = area_level_summer_se[,-1],\n  aes(color = sample_status)\n) +\n  theme_ipsum()\n\n\n\n\n\n\n\n\nWe can also map the outputs of the different models. We observe high agreement between the auxiliary data and the spatial model, with very similar estimated values. These results differ from the intercept-only model, where we observe less variability in the outputs.\n\n# Add geometry\narea_level_summer_gdf &lt;- merge(\n  area_level_summer_est,\n  shape_austria_dis[,c(\"PB\")],\n  by.x = \"Domain\",\n  by.y = \"PB\"\n) %&gt;% \n  st_as_sf()\n\n# Make data long\narea_level_long &lt;- area_level_summer_gdf %&gt;%\n  pivot_longer(\n    cols = c(Intercept_model_est, Covariates_model_est, Spatial_model_est),\n    names_to = \"Model\",\n    values_to = \"Estimate\"\n  )\n\n# Order the models in increasing complexity\narea_level_long$Model &lt;- factor(area_level_long$Model,\n                                levels = c(\"Intercept_model_est\",\n                                           \"Covariates_model_est\",\n                                           \"Spatial_model_est\"))\n\n# Map\nggplot() +\n  geom_sf(data = area_level_long, aes(fill = Estimate), col = NA) +\n  facet_wrap(~Model) +\n  theme_ipsum()"
  },
  {
    "objectID": "workshop/practical/practical.html#footnotes",
    "href": "workshop/practical/practical.html#footnotes",
    "title": "A practical guide on Small Area Estimation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that a domain refers to a small geographic area or a subpopulation (e.g., age group or income class) for which we want to calculate the small area estimation.↩︎"
  },
  {
    "objectID": "workshop/practical/practical.html#unit-level-model",
    "href": "workshop/practical/practical.html#unit-level-model",
    "title": "Mapping the Unseen: Small Area Estimation for Urban Analysis",
    "section": "4.3 Unit-level model",
    "text": "4.3 Unit-level model\nThe last model we will explore in this practical is the unit-level model. This model incorporates both area and unit-level covariates to improve the performance of our estimator.\nThe unit-level model works similarly to the area-level model:\n\nFirst, it calculates a direct estimator of the target variable.\nNext, it fits a model where the direct estimator acts as the dependent variable.\n\nThis time, the model incorporates both area- and unit-level errors:\n\\[\ny_{id} = \\mathbf{x}_{id}^\\top \\boldsymbol{\\beta} + u_d + \\epsilon_{id}\n\\]\nwhere \\(u_d\\) is the area-level error, and \\(\\epsilon_{id}\\) is the unit-level error.\nLike in the area-level model, the unit-level model can make different assumptions about the area-level error distribution. If we assume they are independent and identically distributed, it will compute an IID, whereas if we suspect there might be spatial autocorrelation, we will calculate the BYM2 model.\nFor this example, we will assume that the sample data contains no out-of-sample domains. We need to make this assumption because for out-of-sample domains, we do not have access to individual-level data if we want to use the eusilc data set.\nThe function we will use to calculate the unit-level estimates is smoothUnit(). This function requires an explicit survey design, an object that describes the way the data was collected in the sample. We obtain this object with the svydesign() function. This function allows for both the inclusion of covariates and an adjacency matrix in order to account for spatial effects. In this example, we will calculate the unit-level model with two unit-level covariates.\n\n# Survey design\neusilcA_smp2$id &lt;- 1:nrow(eusilcA_smp2) # Create id column for sample data\ndesign &lt;- svydesign(\n  ids = ~id,\n  weights = ~weight,\n  data = eusilcA_smp2\n)\n\n# Unit level covariates\n# Select unit-level covariates\ncovariates &lt;- c(\"self_empl\", \"tax_adj\")\n\n# Then proceed with scaling the covariates\nXunit_scaled &lt;- eusilcA_smp2 %&gt;%\n  dplyr::select(district = district, all_of(covariates)) %&gt;%\n  mutate(across(all_of(covariates), ~ . / sd(.)))\n\n# Fit model\nfit &lt;- smoothUnit(\n  formula = eqIncome ~ self_empl* + tax_adj,\n  domain = ~district,\n  design = design,\n  X.pop = Xunit_scaled,\n  domain.size = domsize\n)\n\nWarning in smoothUnit(formula = eqIncome ~ self_empl * +tax_adj, domain =\n~district, : No spatial information provided, using iid domain effects\n\n# Estimates\nunit_level &lt;- fit$iid.model.est %&gt;%\n  mutate(\n    mean   = mean * sd1,\n    median = median * sd1,\n    lower  = lower * sd1,\n    upper  = upper * sd1,\n    var    = var * sd1^2,\n    se     = sqrt(var)\n  )\n\nhead(unit_level)\n\n                domain     mean   median     var     lower    upper\n3            Amstetten 15007.64 15004.84 1726421 12432.893 17492.38\n4                Baden 22412.21 22367.88 1488197 20074.218 24870.72\n67             Bludenz 12365.21 12369.45 3817775  9180.365 15986.97\n39      Braunau am Inn 12213.19 12124.77 2040329  9564.889 14961.83\n68             Bregenz 34682.93 34620.89 1775099 32341.177 37231.47\n5  Bruck an der Leitha 22348.54 22361.89 2069353 19804.693 25100.25\n                  method       se\n3  Unit level model: IID 1313.933\n4  Unit level model: IID 1219.917\n67 Unit level model: IID 1953.913\n39 Unit level model: IID 1428.401\n68 Unit level model: IID 1332.328\n5  Unit level model: IID 1438.525\n\n\n\n# # Use model_data\n# model_data &lt;- eusilcA_smp2\n# model_data$id &lt;- 1:nrow(model_data)\n# \n# # Prepare covariates matrix\n# ## Unit-level covariates\n# X_cov &lt;- model_data[, c(\"district\", \"eqIncome\", \"self_empl\", \"rent\",\"weight\")]\n# \n# ## Add area-level covariates\n# X_cov &lt;- merge(X_cov, eusilcA_popAgg[, c(\"Domain\", \"cash\", \"unempl_ben\", \"tax_adj\")],\n#                by.x = \"district\", by.y = \"Domain\", all.y = TRUE)\n# \n# # Scale all numeric variables\n# X_cov_scaled &lt;- X_cov %&gt;%\n#   mutate(across(where(is.numeric) & !matches(\"eqIncome\"), ~ scale(.)[, 1]))\n# \n# # Survey design\n# design &lt;- svydesign(\n#   ids = ~id,\n#   weights = ~weight,\n#   data = X_cov_scaled\n# )\n# \n# # Fit model\n# fit &lt;- smoothUnit(\n#   formula = eqIncome ~ self_empl + tax_adj + cash + unempl_ben,\n#   domain = ~district,\n#   design = design,\n#   X.pop = X_cov_scaled\n# )\n# \n# # Rescale estimates (optional — assumes you saved the original sd of eqIncome as sd1)\n# # If you want to rescale predictions to original units, you need the original sd\n# \n# unit_level &lt;- fit$iid.model.est %&gt;%\n#   mutate(\n#     mean   = mean * sd1,\n#     median = median * sd1,\n#     lower  = lower * sd1,\n#     upper  = upper * sd1,\n#     var    = var * sd1^2,\n#     se     = sqrt(var)\n#   )\n# \n# head(unit_level)"
  },
  {
    "objectID": "workshop/practical/practical.html#area-level-model-1",
    "href": "workshop/practical/practical.html#area-level-model-1",
    "title": "Mapping the Unseen: Small Area Estimation for Urban Analysis",
    "section": "5.1 Area-level Model",
    "text": "5.1 Area-level Model\n\n5.1.1 Intercept-only model\n\n# Direct estimator with untransformed data\ndir_est &lt;- sae::direct(\n  y = eusilcA_smp$eqIncome,\n  dom = eusilcA_smp$district,\n  sweight = eusilcA_smp$weight,\n  domsize = smp_area_size_df,\n  replace = FALSE\n) %&gt;% \n  mutate(Var = SD^2) %&gt;% \n  dplyr::select(Domain, Direct, Var)\n\n# Fit intercept-only Fay-Herriot model (no covariates)\nfh &lt;- sae::mseFH(\n  formula = dir_est$Direct ~ 1,\n  vardir = dir_est$Var\n)\n\n# Extract EBLUPs and RMSEs for sampled domains\nfh_df &lt;- data.frame(\n  Domain = dir_est$Domain,\n  Estimate = fh$est$eblup,\n  Estimator_Type = \"EBLUP\",\n  RMSE = sqrt(fh$mse)\n)\n\n# Extract model intercept and standard error\nintercept &lt;- fh$est$fit$estcoef[\"X\", \"beta\"]\nse &lt;- fh$est$fit$estcoef[\"X\", \"std.error\"]\n\n# Identify out-of-sample domains\noos_domains &lt;- setdiff(eusilcA_popAgg$Domain, dir_est$Domain)\n\n# Create synthetic estimates for OOS domains\noos_estimates &lt;- data.frame(\n  Domain = oos_domains,\n  Estimate = intercept,\n  Estimator_Type = \"Synthetic\",\n  RMSE = se\n)\n\n# Combine sampled and out-of-sample estimates\nfh_area_level_intercept &lt;- bind_rows(fh_df, oos_estimates) %&gt;% \n  arrange(Domain)\n\nhead(fh_area_level_intercept)\n\n               Domain Estimate Estimator_Type     RMSE\n1           Amstetten 16128.32          EBLUP 2217.438\n2               Baden 20228.42          EBLUP 2584.317\n3             Bludenz 14402.31          EBLUP 2333.158\n4      Braunau am Inn 13838.66          EBLUP 1972.521\n5             Bregenz 22122.12          EBLUP 3125.592\n6 Bruck an der Leitha 19955.96          EBLUP 2851.786\n\n\n\n\n5.1.2 Fay-Herriot model with auxiliary variables\n\n# Add more covariates\nX_covar &lt;- merge(dir_est, eusilcA_popAgg[,c(\"Domain\",\"cash\")])\n\n# Fit model with covariates\nfh_cov &lt;- mseFH(\n  formula = Direct ~ cash,\n  vardir = Var,\n  data = X_covar\n)\n\n# Format it as a table\nfh_cov_df &lt;- data.frame(\n  \"Domain\" = dir_est$Domain,\n  EBLUP = fh_cov$est$eblup,\n  RMSE = sqrt(fh_cov$mse)\n)\n\n# Extract parameter estimates\nintercept &lt;- fh_cov$est$fit$estcoef[\"X(Intercept)\", \"beta\"]\nslope_cash &lt;- fh_cov$est$fit$estcoef[\"Xcash\", \"beta\"]\nse_intercept &lt;- fh_cov$est$fit$estcoef[\"X(Intercept)\", \"std.error\"]\n\n# Prepare covariates for all domains\nX_all &lt;- eusilcA_popAgg[, c(\"Domain\", \"cash\")]\n\n# Predict synthetic estimate for all areas\nX_all &lt;- X_all %&gt;%\n  mutate(\n    Estimate = intercept + slope_cash * cash\n  )\n\n# Mark sample status\nX_all$in_sample &lt;- X_all$Domain %in% dir_est$Domain\n\n# Add model EBLUPs and RMSEs for in-sample areas\nX_all &lt;- left_join(X_all, fh_cov_df, by = \"Domain\", suffix = c(\"_synthetic\", \"_eblup\"))\n\n# Combine estimates\nfh_area_level_covariates &lt;- X_all %&gt;%\n  mutate(\n    Final_Estimate = ifelse(!is.na(EBLUP), EBLUP, Estimate),\n    Estimator_Type = ifelse(!is.na(EBLUP), \"EBLUP\", \"Synthetic\"),\n    Final_RMSE = ifelse(!is.na(RMSE), RMSE, se_intercept)\n  ) %&gt;%\n  dplyr::select(Domain, Final_Estimate, Estimator_Type, Final_RMSE) %&gt;% \n  arrange(Domain)\n\n# See output\nhead(fh_area_level_covariates)\n\n               Domain Final_Estimate Estimator_Type Final_RMSE\n1           Amstetten       13226.34          EBLUP   959.1525\n2               Baden       19689.37          EBLUP   780.5883\n3             Bludenz       12294.51          EBLUP   937.6942\n4      Braunau am Inn       12202.68          EBLUP  1119.8066\n5             Bregenz       30613.82          EBLUP  1329.5131\n6 Bruck an der Leitha       22398.59          EBLUP   803.8764\n\n\n\n\n5.1.3 Compare models\n\n# Make sure both dfs are in the same order\nidentical(fh_area_level_intercept$Domain, fh_area_level_covariates$Domain)\n\n[1] TRUE\n\n# Create data frame for comparison\nfh_comparison &lt;- data.frame(\n  Domain = fh_area_level_intercept$Domain,\n  Intercept_model_est = fh_area_level_intercept$Estimate,\n  Covariates_model_est = fh_area_level_covariates$Final_Estimate\n)\n\n# Flag out-of-sample domains\noos &lt;- setdiff(eusilcA_pop$district,eusilcA_smp$district)\nfh_comparison &lt;- fh_comparison %&gt;%\n  mutate(sample_status = if_else(Domain %in% oos, \"out-of-sample\", \"in-sample\"))\n\n# Plot\nggpairs(\n  data = fh_comparison[,-1],\n  aes(color = sample_status)\n) +\n  theme_ipsum()\n\nWarning in cor(x, y): the standard deviation is zero\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "workshop/practical/practical.html#understanding-the-sampling-weights",
    "href": "workshop/practical/practical.html#understanding-the-sampling-weights",
    "title": "A practical guide on Small Area Estimation",
    "section": "3.1 Understanding the sampling weights",
    "text": "3.1 Understanding the sampling weights\nBefore we manually calculate the direct estimator, we will have a closer look at the sampling weights (\\(w_{di}\\)). These values represent the survey weigths, and we can find them in the weights column of our sample data set eusilcA_smp. In our survey, the weights are calculated as the inverse probabilities of selection or, in other words, the inverse of the likelyhood of an individual of the population being sampled. The value indicates the number of survey respondents in the population. This information can usually be found in the documentation of the survey, together with any clusters or strata that might have been defined by the surveyors.\nThese design weights can be calculated following this formula:\n\\[\nweight_i = \\frac{N_d}{n_d}\n\\]\nTo manually calculate the weights, we can do the following:\n\n# Check that each district has different weights assigned\n# Count number of times each weigth repeats itself in the sample (in total we should get 70 rows)\nweights_per_district &lt;- eusilcA_smp |&gt; \n  select(district, weight) |&gt; \n  distinct()\nnrow(weights_per_district)\n\n[1] 70\n\n# Calculate the weights manually\n## Count population per district\npop_count &lt;- eusilcA_pop |&gt;\n  count(district, name = \"N_d\")\n\n## Count sample per district\nsmp_count &lt;- eusilcA_smp |&gt;\n  count(district, name = \"n_d\")\n\n## Merge and calculate weight\nweight_check &lt;- left_join(smp_count, pop_count, by = \"district\") |&gt;\n  mutate(calculated_weight = N_d / n_d)\n\n## Add weights from sample and check they are the same\nweight_check &lt;- weight_check |&gt; \n  left_join(weights_per_district, by = \"district\")\n\n## Plot to check if they are the same\nggplot() +\n  geom_point(data = weight_check, aes(x = calculated_weight, y = weight)) +\n  labs(\n    title = \"Calculated Weights vs Survey Weights\",\n    x = \"Calculated Weights\",\n    y = \"Survey Weights\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows that the manually calculated weights match the survey weights."
  },
  {
    "objectID": "workshop/practical/practical.html#manual-calculation-of-the-direct-estimator",
    "href": "workshop/practical/practical.html#manual-calculation-of-the-direct-estimator",
    "title": "A practical guide on Small Area Estimation",
    "section": "3.2 Manual calculation of the direct estimator",
    "text": "3.2 Manual calculation of the direct estimator\nNow that we understand Let us now calculate the direct estimator manually. We will follow the Horvitz-Thompson estimator formula, which we introduced above. The steps are as follows:\n\n# Calculate total population values for sampled domains\nN &lt;- pop_count |&gt; filter(pop_count$district %in% eusilcA_smp$district)\n\n# Add N to the sample data\ndir_df &lt;- eusilcA_smp |&gt; \n  left_join(N, by = \"district\") |&gt; \n  dplyr::select(district, eqIncome, weight, N_d)\n\n# Calculate direct estimator manually for each domain\nmanual_direct &lt;- dir_df |&gt; \n  mutate(w_Y = weight * eqIncome) |&gt; \n  group_by(district, N_d) |&gt; \n  summarise(sum_wY = sum(w_Y, na.rm = TRUE), .groups = \"drop\") |&gt; \n  mutate(dir_est_manual = sum_wY / N_d)\n\n# See results\nhead(manual_direct)\n\n# A tibble: 6 × 4\n  district              N_d    sum_wY dir_est_manual\n  &lt;chr&gt;               &lt;int&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n1 Amstetten             321  4879568.         15201.\n2 Baden                 398  9122834.         22922.\n3 Bludenz               162  1955274.         12070.\n4 Braunau am Inn        282  3437608.         12190.\n5 Bregenz               338 12077145.         35731.\n6 Bruck an der Leitha   265  6294628.         23753.\n\n\nThe values in the dir_est_manual column represent the direct estimator of the mean equivalised income for each district. The values are calculated by summing the product of the sample weights and the target variable, and then dividing by the population size of each district."
  },
  {
    "objectID": "workshop/practical/practical.html#direct-estimator-using-the-sae-package",
    "href": "workshop/practical/practical.html#direct-estimator-using-the-sae-package",
    "title": "A practical guide on Small Area Estimation",
    "section": "3.3 Direct estimator using the sae package",
    "text": "3.3 Direct estimator using the sae package\nNow, we will calculate the direct estimator using the sae package. The direct() function computes the Horvitz-Thompson estimator, the same one we have just manually computed. In addition to the direct estimator of the mean, the direct() function also gives us the standard deviation and coefficient of variation for each domain.\n\n# Calculate the direct estimator manually\nsae_direct &lt;- sae::direct(\n  y = eusilcA_smp$eqIncome,        # Individual values of the target variable\n  dom = eusilcA_smp$district,      # Domain names\n  sweight = eusilcA_smp$weight,    # Sampling weights\n  domsize = N,                     # Data frame with domain names and the corresponding population sizes.\n  replace = FALSE                  # Sampling conducted without replacement\n)\n\n# See results\nhead(sae_direct)\n\n                Domain SampSize   Direct       SD       CV\n3            Amstetten       33 15201.15 2723.329 17.91528\n4                Baden       40 22921.69 3564.496 15.55075\n67             Bludenz       17 12069.59 2956.478 24.49526\n39      Braunau am Inn       29 12190.10 2294.239 18.82051\n68             Bregenz       34 35731.20 6084.361 17.02815\n5  Bruck an der Leitha       27 23753.31 4475.512 18.84163\n\n\nOnce we have calculated the direct estimator manually and using the pre-defined direct() function of the sae package, we can compare the results\n\n\n\n\n\n\n\n\n\nThe plot shows a perfect match between the manually computed direct estimator values and the values from the direct() function of the sae package."
  },
  {
    "objectID": "workshop/practical/practical.html#making-sense-of-the-results",
    "href": "workshop/practical/practical.html#making-sense-of-the-results",
    "title": "A practical guide on Small Area Estimation",
    "section": "3.4 Making sense of the results",
    "text": "3.4 Making sense of the results\nThe direct() function has given us the direct esitmates of the equivalised income, and the standard deviation for each district. With this information, we can build a confidence interval for our estimates. This is an important advantage of regression-based SAE methods over other alternatives such as microsimulation, since this allows us to determine how precise our estimates are.\nWe will now plot our estimates with their associated 95% confidence intervals:\n\n# Add confidence intervals\nsae_direct_ci &lt;- sae_direct |&gt; \n  mutate(\n    lower = Direct - 1.96 * SD,\n    upper = Direct + 1.96 * SD\n  )\n\n# Plot\nggplot(sae_direct_ci, aes(x = reorder(Domain, Direct), y = Direct)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), colour = \"gray\", width = 0.3) +\n  geom_point(color = \"#264653\") +\n  coord_flip() +\n  labs(\n    x = \"District\",\n    y = \"Equivalised income (Direct Estimate)\",\n    title = \"Direct Estimates of Equivalised Income (95% CI)\"\n  ) +\n  theme_minimal() +\n  theme(\n  axis.text.y = element_text(lineheight = 1.5)  # Default is 1, try 1.3–2\n)\n\n\n\n\n\n\n\n\nThe plot shows the value of the direct estimator and the confidence interval around that value. The confidence interval is calculated as the mean plus and minus the standard deviation multiplied by a constant value (1.96) that comes from the standard normal distribution at a 95% confidence interval. The interval represents all the possible ``true values’’ of our estimator. Longer error bars indicate higher uncertainty around the values –there is a larger range of potential true values– while shorter error bars indicate lower uncertainty –the range of potential values is more constrained.\nThe advantage of using the sae package is that it allows us to easily implement variations of the direct estimate calculation. In some cases, the survey might have been conducted following different surveying strategies in a more flexible way. For instance, it might be that the sampling was conducted with replacement, instead of without replacement [^2], or it could be that we do not have access to the domain of interest population sizes, in which case we would have to proceed differently to calculate our direct es\n[^2] Survey sampling –selecting individuals from the total population for a survey– can be done with or without replacement. Sampling without replacement means that individuals from a domain can only be sampled once (imagine we take a ball out of a bag and we do not put it back); or with replacement (we take a ball out of the bag and put it back)."
  },
  {
    "objectID": "workshop/practical/practical.html#making-sense-of-the-results-understanding-uncertainty",
    "href": "workshop/practical/practical.html#making-sense-of-the-results-understanding-uncertainty",
    "title": "A practical guide on Small Area Estimation",
    "section": "3.4 Making sense of the results – Understanding uncertainty",
    "text": "3.4 Making sense of the results – Understanding uncertainty\nThe direct() function has given us the direct esitmates of the equivalised income, and the standard deviation for each district. With this information, we can build a confidence interval for our estimates. This is an important advantage of regression-based SAE methods over other alternatives such as microsimulation, since this allows us to determine how precise our estimates are.\nWe will now plot our estimates with their associated 95% confidence intervals:\n\n# Add confidence intervals\nsae_direct_ci &lt;- sae_direct |&gt; \n  mutate(\n    lower = Direct - 1.96 * SD,\n    upper = Direct + 1.96 * SD\n  )\n\n# Plot\nggplot(sae_direct_ci, aes(x = reorder(Domain, Direct), y = Direct)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), colour = \"gray\", width = 0.3) +\n  geom_point(color = \"#264653\") +\n  coord_flip() +\n  labs(\n    x = \"District\",\n    y = \"Equivalised income (Direct Estimate)\",\n    title = \"Direct Estimates of Equivalised Income (95% CI)\"\n  ) +\n  theme_minimal() +\n  theme(\n  axis.text.y = element_text(lineheight = 1.5)  # Default is 1, try 1.3–2\n)\n\n\n\n\n\n\n\n\nThe plot shows the value of the direct estimator and the confidence interval around that value. The confidence interval is calculated as the mean plus and minus the standard deviation multiplied by a constant value (1.96) that comes from the standard normal distribution at a 95% confidence interval. The interval represents all the possible “true values’’ of our estimator. Longer error bars indicate higher uncertainty around the estimates –there is a larger range of potential true values– while shorter error bars indicate lower uncertainty –the range of potential values is more constrained.\nThe advantage of using the sae package is that it allows us to easily implement variations of the direct estimate calculation. In some cases, the survey might have been conducted following different surveying strategies in a more flexible way. For instance, it might be that the sampling was conducted with replacement, instead of without replacement [^2], or it could be that we do not have access to the domain of interest population sizes, in which case we would have to proceed differently to calculate our direct es\n[^2] Survey sampling –selecting individuals from the total population for a survey– can be done with or without replacement. Sampling without replacement means that individuals from a domain can only be sampled once (imagine we take a ball out of a bag and we do not put it back); or with replacement (we take a ball out of the bag and put it back)."
  },
  {
    "objectID": "workshop/practical/practical.html#the-fay-herriot-model-and-our-austria-example",
    "href": "workshop/practical/practical.html#the-fay-herriot-model-and-our-austria-example",
    "title": "A practical guide on Small Area Estimation",
    "section": "4.1 The Fay-Herriot Model and Our Austria Example",
    "text": "4.1 The Fay-Herriot Model and Our Austria Example\nThe core idea of the Fay-Herriot model is to combine the unreliable direct estimator (the sampling model) with a more stable, model-based estimate (the linking model).\n1. The Sampling Model (Revisited with the example)\n\\[\\hat{\\theta}_i^{\\mathrm{DIR}} = \\theta_i + \\epsilon_i\\]\n\n\\(\\hat{\\theta}_i^{\\mathrm{DIR}}\\): Our direct estimate of average income for district \\(i\\) (e.g., €56,667 for Murau, calculated from the small sample).\n\\(\\theta_i\\): The true, but unknown, average income for district \\(i\\).\n\\(\\epsilon_i\\): The sampling error, which is large for districts with small samples (like Murau) and small for districts with large samples (like Wien). The variance of this error, \\(V_i\\), is known from our survey design.\n\n\n4.1.1 2. The Linking Model: The “Secret Sauce”\nThis is where we “borrow strength” from other areas and auxiliary data. The linking model assumes that the true average income (\\(\\theta_i\\)) for a district is not completely random but is related to other known characteristics of that district.\n\\[\\theta_i = x_i' \\beta + u_i\\]\n\n\\(\\theta_i\\): The true average income for district \\(i\\) (e.g., Murau). We’re trying to model this value.\n\\(x_i' \\beta\\): This is the model-based part. It’s a linear regression that predicts the average income based on known, reliable data for each district. These are our “auxiliary variables” (\\(x_i\\)).\n\n\\(x_i\\): A vector of auxiliary variables for district \\(i\\). For our Austrian example, these could be:\n\nThe average education level of the district.\nThe average age of the population.\nThe unemployment rate from a separate census-like source.\nThe proportion of the population working in specific industries (e.g., agriculture vs. manufacturing).\n\n\\(\\beta\\): A vector of regression coefficients. These are unknown parameters that the model estimates from the data. They tell us how much each auxiliary variable affects the average income across all districts. For example, a positive \\(\\beta\\) for “average education level” would mean that districts with higher education tend to have higher average incomes.\n\n\\(u_i\\): This is the area-specific random effect. This is a very important part of the model.\n\nIt represents the part of the true average income (\\(\\theta_i\\)) that cannot be explained by the auxiliary variables (\\(x_i\\)). It’s the unique, un-modeled characteristic of district \\(i\\).\nFor Murau, even after accounting for its demographics and industries, there might be unique local factors that make its true average income higher or lower than what the model predicts. The random effect \\(u_{\\text{Murau}}\\) captures this unique difference.\nThe model assumes these effects are normally distributed with a mean of zero and a common variance \\(\\sigma_u^2\\). This means that on average, the auxiliary variables do a good job of predicting the income, but there is always some random, unique variation from district to district.\n\n\n\n\n4.1.2 The Error Terms: \\(\\epsilon_i\\) vs. \\(u_i\\)\nThis is a critical distinction in the Fay-Herriot model.\n\n\\(\\epsilon_i\\) (Sampling Error): This is the error in our measurement. It’s the random variability that comes from our survey’s sampling process. This error is specific to the direct estimator.\n\nExample: The error in our Murau direct estimate is huge because of the tiny sample. The variance \\(V_{\\text{Murau}}\\) is large.\n\n\\(u_i\\) (Area-Specific Random Effect): This is the error in our linking model. It’s the random variability that comes from our regression model not perfectly explaining the true income. This error is specific to the underlying true value \\(\\theta_i\\).\n\nExample: Even if we had a perfect census for all districts, a simple regression model wouldn’t perfectly predict every district’s average income. The random effect \\(u_i\\) accounts for that inherent, unmodeled variability.\n\n\n\n\n4.1.3 The Advanced Error Term: Spatial Correlation\nThe final part of your description introduces a more advanced version of the linking model, which addresses a key limitation of the basic model.\n\\[u = \\rho_1 Wu + \\epsilon\\]\n\nBasic Assumption: The standard Fay-Herriot model assumes that the unique effects (\\(u_i\\)) for each district are independent. This means that a high true income in one district (after accounting for the auxiliary variables) tells us nothing about the true income in a neighboring district. This assumption is often wrong.\nThe Reality: In our Austria example, it’s very likely that a district’s true average income is correlated with its neighbors’ incomes. If a district has a higher-than-expected income, its neighboring districts are also likely to have higher-than-expected incomes due to shared economic factors, commuter patterns, or other regional influences. This is called spatial correlation.\nThe Advanced Model: The equation you provided is a way to model this spatial correlation.\n\n\\(W\\): This is an adjacency matrix. It’s a mathematical representation of which districts are neighbors. For example, if Wien and Graz are neighbors, the matrix would have a 1 in the corresponding cell; otherwise, it would have a 0.\n\\(\\rho_1\\): This is a spatial autoregression parameter. It’s a single value between -1 and 1 that tells us the strength of the spatial correlation.\n\nIf \\(\\rho_1\\) is positive, it means that districts with a high random effect tend to be next to other districts with high random effects.\nIf \\(\\rho_1\\) is close to 0, there is no spatial correlation, and the model simplifies to the basic Fay-Herriot model.\n\nThe new error term: The new error term, \\(\\epsilon\\), is now assumed to be independent and identically distributed, and it is part of this new equation for \\(u\\).\n\n\nIn summary: The linking model provides a way to estimate the true average income for each district by combining a regression model with auxiliary variables and a unique, random effect. The advanced version of this model acknowledges and quantifies the spatial correlation between neighboring districts, which often provides a more accurate and robust estimate, especially for areas with very small sample sizes.\nIntercept only model\nThis is an excellent description of the simplest form of the Fay-Herriot model. It helps us understand the fundamental principle of “borrowing strength” without the added complexity of auxiliary variables. Let’s break down the intercept-only model using our Austrian districts and income example.\n\n\n4.1.4 The Intercept-Only Model in Simple Terms\nThe intercept-only model is the most basic way to improve a direct estimator. It essentially says:\n\n“We don’t have any specific data (like demographics or education) to predict a district’s income.”\n“Therefore, our best guess for any district’s true income is the overall average income for all of Austria.”\n“However, we know that each district will have its own unique, random deviation from this average.”\n“We will combine this overall average with our direct estimate from the survey, giving more weight to the overall average when the direct estimate is unreliable (i.e., has a high variance).”"
  },
  {
    "objectID": "workshop/practical/practical.html#area-level-models-the-fay-herriot-model",
    "href": "workshop/practical/practical.html#area-level-models-the-fay-herriot-model",
    "title": "A practical guide on Small Area Estimation",
    "section": "4.1 Area-level models – The Fay-Herriot model",
    "text": "4.1 Area-level models – The Fay-Herriot model\nThe area-level model (also known as the Fay-Herriot model) incorporates auxiliary data at domain level to improve the direct estimator. The core idea of this model is that it combines the unreliable estimates from the direct estimator with more reliable, model-based estimate. To do so, it combines two models: the sampling model (the direct estimator) and the linking model (the model-based estimate).\n\n4.1.1 The sampling model\nThe sampling model is nothing else but the direct estimator:\n\\[\n\\hat{\\theta}_i^{\\mathrm{DIR}} = \\theta_i + \\epsilon_i\n\\]\n\n\\(\\hat{\\theta}_i^{\\mathrm{DIR}}\\): Our direct estimate of average income for district \\(i\\)..\n\\(\\theta_i\\): The true, but unknown, average income for district \\(i\\).\n\\(\\epsilon_i\\): The sampling error, which is large for districts with small samples and small for districts with large samples (remember the plot with the confidence intervals). The variance of this error, \\(V_i\\), is known from our survey design.\n\n\n\n4.1.2 2. The Linking Model\nThis is where we “borrow strength” from other areas using auxiliary data. The linking model assumes that the true average income (\\(\\theta_i\\)) for a district is not completely random but is related to other known characteristics of that district (the auxiliary variables \\(x_i'\\)).\n\\[\\theta_i = x_i' \\beta + u_i\\]\n\n\\(\\theta_i\\): The true average income for district \\(i\\). This is the value we want to model.\n\\(x_i' \\beta\\): This is the model-based part. It’s a linear regression that predicts the average income based on known, reliable data for each district.\n\n\\(x_i\\): A vector of auxiliary variables for district \\(i\\). For our Austrian example, these could be:\n\nThe average education level of the district.\nThe average age of the population.\nThe unemployment rate from a separate census-like source.\nThe proportion of the population working in specific industries (e.g., agriculture vs. manufacturing).\n\n\\(\\beta\\): A vector of regression coefficients. These are unknown parameters that the model estimates from the data. They tell us how much each auxiliary variable affects the average income across all districts. For example, a positive \\(\\beta\\) for “average education level” would mean that districts with higher education tend to have higher average incomes.\n\n\\(u_i\\): This is the area-specific random effect. This is a very important part of the model.\n\nIt represents the part of the true average income (\\(\\theta_i\\)) that cannot be explained by the auxiliary variables (\\(x_i\\)). It’s the unique, un-modeled characteristic of district \\(i\\).\nFor Murau, even after accounting for its demographics and industries, there might be unique local factors that make its true average income higher or lower than what the model predicts. The random effect \\(u_{\\text{Murau}}\\) captures this unique difference.\nThe model assumes these effects are normally distributed with a mean of zero and a common variance \\(\\sigma_u^2\\). This means that on average, the auxiliary variables do a good job of predicting the income, but there is always some random, unique variation from district to district.\n\n\n\n\n4.1.3 The Error Terms: \\(\\epsilon_i\\) vs. \\(u_i\\)\nThis is a critical distinction in the Fay-Herriot model.\n\n\\(\\epsilon_i\\) (Sampling Error): This is the error in our measurement. It’s the random variability that comes from our survey’s sampling process. This error is specific to the direct estimator.\n\nExample: The error in our Murau direct estimate is huge because of the tiny sample. The variance \\(V_{\\text{Murau}}\\) is large.\n\n\\(u_i\\) (Area-Specific Random Effect): This is the error in our linking model. It’s the random variability that comes from our regression model not perfectly explaining the true income. This error is specific to the underlying true value \\(\\theta_i\\).\n\nExample: Even if we had a perfect census for all districts, a simple regression model wouldn’t perfectly predict every district’s average income. The random effect \\(u_i\\) accounts for that inherent, unmodeled variability.\n\n\n\n\n4.1.4 The Advanced Error Term: Spatial Correlation\nThe final part of your description introduces a more advanced version of the linking model, which addresses a key limitation of the basic model.\n\\[u = \\rho_1 Wu + \\epsilon\\]\n\nBasic Assumption: The standard Fay-Herriot model assumes that the unique effects (\\(u_i\\)) for each district are independent. This means that a high true income in one district (after accounting for the auxiliary variables) tells us nothing about the true income in a neighboring district. This assumption is often wrong.\nThe Reality: In our Austria example, it’s very likely that a district’s true average income is correlated with its neighbors’ incomes. If a district has a higher-than-expected income, its neighboring districts are also likely to have higher-than-expected incomes due to shared economic factors, commuter patterns, or other regional influences. This is called spatial correlation.\nThe Advanced Model: The equation you provided is a way to model this spatial correlation.\n\n\\(W\\): This is an adjacency matrix. It’s a mathematical representation of which districts are neighbors. For example, if Wien and Graz are neighbors, the matrix would have a 1 in the corresponding cell; otherwise, it would have a 0.\n\\(\\rho_1\\): This is a spatial autoregression parameter. It’s a single value between -1 and 1 that tells us the strength of the spatial correlation.\n\nIf \\(\\rho_1\\) is positive, it means that districts with a high random effect tend to be next to other districts with high random effects.\nIf \\(\\rho_1\\) is close to 0, there is no spatial correlation, and the model simplifies to the basic Fay-Herriot model.\n\nThe new error term: The new error term, \\(\\epsilon\\), is now assumed to be independent and identically distributed, and it is part of this new equation for \\(u\\).\n\n\nIn summary: The linking model provides a way to estimate the true average income for each district by combining a regression model with auxiliary variables and a unique, random effect. The advanced version of this model acknowledges and quantifies the spatial correlation between neighboring districts, which often provides a more accurate and robust estimate, especially for areas with very small sample sizes.\nIntercept only model\nThis is an excellent description of the simplest form of the Fay-Herriot model. It helps us understand the fundamental principle of “borrowing strength” without the added complexity of auxiliary variables. Let’s break down the intercept-only model using our Austrian districts and income example.\n\n\n4.1.5 The Intercept-Only Model in Simple Terms\nThe intercept-only model is the most basic way to improve a direct estimator. It essentially says:\n\n“We don’t have any specific data (like demographics or education) to predict a district’s income.”\n“Therefore, our best guess for any district’s true income is the overall average income for all of Austria.”\n“However, we know that each district will have its own unique, random deviation from this average.”\n“We will combine this overall average with our direct estimate from the survey, giving more weight to the overall average when the direct estimate is unreliable (i.e., has a high variance).”\n\n\n# 1) Prepare the sample data (rename columns for consistency)\nsmpAgg &lt;- sae_direct %&gt;%\n  mutate(Var_Direct = SD^2,       # variance of the direct estimate\n         Mean = Direct) %&gt;%       # rename Direct to Mean\n  select(Domain, Mean, Var_Direct)\n\n# 2) Prepare the population covariates (all 94 domains)\npopAgg &lt;- eusilcA_popAgg %&gt;%\n  select(Domain, cash, self_empl)\n\n# 3) Combine sample and population data\ncombined_data &lt;- combine_data(\n  pop_data   = popAgg,    pop_domains = \"Domain\",\n  smp_data   = smpAgg,    smp_domains = \"Domain\"\n)\n\nNon-sampled domains exist.\n\n# 4) Fit the Fay–Herriot model\nfh_fit &lt;- emdi::fh(\n  fixed         = Mean ~ 1,\n  vardir        = \"Var_Direct\",\n  combined_data = combined_data,\n  domains       = \"Domain\",\n  method        = \"ml\",              \n  MSE           = TRUE\n)\n\nPlease note that the model selection criteria are only computed based on the in-sample domains.\n\n# 5) Extract estimates (includes OOS areas)\nresults &lt;- as.data.frame(emdi::estimators(fh_fit, MSE = TRUE))\n\n# Plot results and 95% confidence intervals\n\nresults_ci &lt;- results %&gt;%\n  mutate(\n    lower = FH - 1.96 * sqrt(FH_MSE),\n    upper = FH + 1.96 * sqrt(FH_MSE)\n  )\n\n# 2) Plot\nggplot(results_ci, aes(x = reorder(Domain, FH), y = FH)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), colour = \"gray\", width = 0.3) +\n  geom_point(color = \"#264653\") +\n  coord_flip() +\n  labs(\n    x = \"District\",\n    y = \"Equivalised income (Fay–Herriot estimate)\",\n    title = \"Model-Based Estimates of Equivalised Income (95% CI)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(lineheight = 1.5))\n\n\n\n\n\n\n\n\n\n\n4.1.6 The Fay-Herriot model with auxiliary variables\n\n# 1) Prepare the sample data (rename columns for consistency)\nsmpAgg &lt;- sae_direct %&gt;%\n  mutate(Var_Direct = SD^2,       # variance of the direct estimate\n         Mean = Direct) %&gt;%       # rename Direct to Mean\n  select(Domain, Mean, Var_Direct)\n\n# 2) Prepare the population covariates (all 94 domains)\npopAgg &lt;- eusilcA_popAgg %&gt;%\n  select(Domain, cash, self_empl)\n\n# 3) Combine sample and population data\ncombined_data &lt;- combine_data(\n  pop_data   = popAgg,    pop_domains = \"Domain\",\n  smp_data   = smpAgg,    smp_domains = \"Domain\"\n)\n\nNon-sampled domains exist.\n\n# 4) Fit the Fay–Herriot model\nfh_fit &lt;- emdi::fh(\n  fixed         = Mean ~ cash + self_empl,\n  vardir        = \"Var_Direct\",\n  combined_data = combined_data,\n  domains       = \"Domain\",\n  method        = \"ml\",              \n  MSE           = TRUE\n)\n\nPlease note that the model selection criteria are only computed based on the in-sample domains.\n\n\nWarning in emdi::fh(fixed = Mean ~ cash + self_empl, vardir = \"Var_Direct\", :\nThe estimate of the variance of the random effects falls at the interval limit.\nIt is recommended to choose a larger interval for the estimation of the\nvariance of the random effects (specify interval input argument).\n\n# 5) Extract estimates (includes OOS areas)\nresults &lt;- as.data.frame(emdi::estimators(fh_fit, MSE = TRUE))\n\nWarning in FUN(X[[i]], ...): NaNs produced\n\n# Plot results and 95% confidence intervals\n\nresults_ci &lt;- results %&gt;%\n  mutate(\n    lower = FH - 1.96 * sqrt(FH_MSE),\n    upper = FH + 1.96 * sqrt(FH_MSE)\n  )\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `lower = FH - 1.96 * sqrt(FH_MSE)`.\nCaused by warning in `sqrt()`:\n! NaNs produced\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n# 2) Plot\nggplot(results_ci, aes(x = reorder(Domain, FH), y = FH)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), colour = \"gray\", width = 0.3) +\n  geom_point(color = \"#264653\") +\n  coord_flip() +\n  labs(\n    x = \"District\",\n    y = \"Equivalised income (Fay–Herriot estimate)\",\n    title = \"Model-Based Estimates of Equivalised Income (95% CI)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(lineheight = 1.5))"
  },
  {
    "objectID": "workshop/practical/practical.html#unit-level-models-the-battese-harter-fuller-model",
    "href": "workshop/practical/practical.html#unit-level-models-the-battese-harter-fuller-model",
    "title": "A practical guide on Small Area Estimation",
    "section": "4.2 Unit-level models – The Battese-Harter-Fuller model",
    "text": "4.2 Unit-level models – The Battese-Harter-Fuller model\n\nebp_model &lt;- ebp(\n  fixed        = eqIncome ~  cash + self_empl, # model formula\n  pop_data     = eusilcA_pop,    # population frame\n  pop_domains  = \"district\",       # domain variable in population data\n  smp_data     = eusilcA_smp,    # sample microdata\n  smp_domains  = \"district\",       # domain variable in sample data\n  L            = 100,            # number of Monte Carlo simulations\n  MSE          = TRUE,           # request MSEs\n  B            = 50,             # bootstrap reps for MSE\n  transformation = \"box.cox\",    # transformation for skewed income data\n  cpus         = 2               # parallel cores (optional)\n)\n\nThe threshold for the HCR and the PG is automatically set to 60% of the median of the dependent variable and equals 10924.32\n\n\n\nBootstrap started                                            \n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\n\n\nBootstrap completed\n\n\n\nsummary(ebp_model)                  # model + data info\n\nEmpirical Best Prediction\n\nCall:\n ebp(fixed = eqIncome ~ cash + self_empl, pop_data = eusilcA_pop, \n    pop_domains = \"district\", smp_data = eusilcA_smp, smp_domains = \"district\", \n    L = 100, transformation = \"box.cox\", MSE = TRUE, B = 50, \n    cpus = 2)\n\nOut-of-sample domains:  24 \nIn-sample domains:  70 \n\nSample sizes:\nUnits in sample:  1945 \nUnits in population:  25000 \n                   Min. 1st Qu. Median      Mean 3rd Qu. Max.\nSample_domains       14    17.0   22.5  27.78571   29.00  200\nPopulation_domains    5   126.5  181.5 265.95745  265.75 5857\n\nExplanatory measures:\n Marginal_R2 Conditional_R2\n   0.1419789      0.4519884\n\nResidual diagnostics:\n               Skewness Kurtosis Shapiro_W    Shapiro_p\nError         0.4968720 5.757915 0.9700517 1.052692e-19\nRandom_effect 0.7607851 3.470813 0.9572161 1.761680e-02\n\nICC:  0.3613075 \n\nTransformation:\n Transformation Method Optimal_lambda Shift_parameter\n        box.cox   reml      0.3944461               0\n\nresults &lt;- as.data.frame(emdi::estimators(ebp_model, MSE=TRUE)) # domain estimates + MSE\n\nebp_ci &lt;- results %&gt;%\n  mutate(\n    lower = Mean - 1.96 * sqrt(Mean_MSE),\n    upper = Mean + 1.96 * sqrt(Mean_MSE)\n  )\n\n# 3) Plot (taller figure so 94 areas aren’t cramped)\np &lt;- ggplot(ebp_ci, aes(x = reorder(Domain, Mean), y = Mean)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), colour = \"gray50\", width = 0.3) +\n  geom_point(color = \"#264653\", size = 2) +\n  coord_flip() +\n  labs(\n    x = \"District\",\n    y = \"Equivalised income (EBP estimate)\",\n    title = \"Unit-level EBP Estimates with 95% Confidence Intervals\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_text(lineheight = 1.5))\n\np"
  }
]