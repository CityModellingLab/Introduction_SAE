---
title: "Small Area Population Estimation<br> <span style='font-size:.5em; font-weight: 500'>A practical guide</span>"
subtitle: "<br><span style='color:#525252; font-size:1.0em;'>British Society for Population Studies 2025 Conference</span>"
format: 
  revealjs:
    transition: fade
    theme: slides-theme.scss
    css: style.css
    footer: "[]()"
    mainfont: "Avenir Book"
bibliography: bibliography.bib
nocite: |
  @*  
---

# Introduction

## About us

:::::: columns
:::: {.column width="49%"}
![](Figures/Adam.jpg)
:::


::: {.column width="49%"}
![](Figures/Clara.jpg)
:::
::::::

------------------------------------------------------------------------

## About TRACK-UK

::: {.incremental style="font-size: 80%;"}
-   This workshop is part of a wider project we are working at at UCL, together with Arup and the Alan Turing Institute.

-   The project is called TRACK-UK, and funded by the ESRC.

-   The project was inspired by the fact that we usually have limited understanding of what happens at small area level in terms of mobility, travel emissions, energy use, or housing quality.

-   There is a wide range of existing surveys (NTS, English Housing Survey), but they are usually designed to be used at regional or national scale. It is not easy to use this data to produce neighbourhood or small area estimates (OA, MSOA, LSOA).

-   As part of the project, we have had to learn about these methods, so we take this workshop as an opportunity to share what we have learnt with the urban analytics community.
:::

------------------------------------------------------------------------


## About this workshop

-   The workshop is structured in two parts:

    -   **Theoretical presentation:** We will learn about small area estimation methods from a theoretical perspective, providing an intuitive overview on these methods (9:00 - 9:45)

    -   **Practical**: Hands-on session in `R` where we will see how to apply all these concepts to real data (10:00 - 10:45)


------------------------------------------------------------------------

![](https://t3.ftcdn.net/jpg/04/87/13/32/360_F_487133202_AoYev86tqkqK6IsRjnuFWWsTbPDJtqJa.jpg){fig-align="center" width="350"}

::: notes
-   I have very recently started learning about these methods, so I am still very new to all of this.

-   These methods have been applied in other disciplines for quite a long time (epidemiology, health), but in the context of urban analytics, we haven't widely embraced them. So this is an opportunity to learn a bit more about a very interesting method for us Geography or spatial analysts.

-   We will be talking about some relatively complex statistical concepts with which we might not be very familiar with, but are important to understand what lays behind small area estimation.

:::

------------------------------------------------------------------------

## Expected takeaways

::: incremental
-   Understand what Small Area Estimation is and why it can be useful in the context of Urban Analytics.
-   Get familiar with the main types of SAE methods -- direct estimators, area-level models, unit-level models, and how they differ in terms of data and complexity.
-   Learn how model-based methods can help us improve our estimates by incorporating auxiliary information.
-   Gain hands-on experience in `R` applying these methods to real data.
:::

------------------------------------------------------------------------


# Background

## SAE in the wild

![<span style="font-size:65%;">Source: Bennett, J. E., Rashid, T., Zolfaghari, A., Doyle, Y., Suel, E., Pearson-Stuttard, J.,  & Ezzati, M. (2023). Changes in life expectancy and house prices in London from 2002 to 2019. *The Lancet Regional Healthâ€“Europe*, 27.</span>](Figures/housing-prices.jpg){fig-align="center" width="4000"}


::: notes
-   In other fields, like epidemiology or health, we see maps like this, where metrics are provided at very high spatial resolution level.

-   These maps are part of a paper that evaluated the link between housing prices and life expectancy in London between 2002 and 2019 for all London's LSOAs.

-   Such granular outputs allow for more nuanced understanding of inequalities.
:::



------------------------------------------------------------------------

## SAE in the wild


![<span style="font-size:80%;">Source: ONS.</span>](Figures/ONS_income_SAE.png){fig-align="center" width="4000"}


::: notes
-   Official institutions like the Office for National Statistics also use these methods to produce very granular statistics.

-   This other map shows the estimates that the Office for National Statistics produces for income at postcode level.

-   We believe there is a lot of potential for using these methods within our community, adn would like to use this workshop to spread the word and share what we've learnt about them so far.
:::



## Clearing up the confusion

```{=html}
<div class="custom-grid-stacked">

  <div class="square-row">
    <div class="rectangle rect1" onclick="toggleText(this)">
      <div class="title">Dasymetric population</div>
      <div class="content">
           <strong>Dasymetric mapping</strong> is used to disaggregate coarse resolution population estimates (e.g. census) to produce a finer resolution estimate of a population.
      </div>
    </div>

    <div class="rectangle rect2" onclick="toggleText(this)">
      <div class="title">Synthetic Population</div>
      <div class="content">
        <strong>Synthetic populations</strong> use statistical processes to simulate individual-level data that match known population totals and distributions from sources like census.
      </div>
    </div>
  </div>

  <div class="square-row center">
    <div class="rectangle rect3" onclick="toggleText(this)">
      <div class="title">Small Area Estimation</div>
      <div class="content">
        <strong>Small Area Estimation</strong> is a a statistical methodology that uses models to produce reliable estimates for areas where survey samples are too small or even zero.
      </div>
    </div>
  </div>

</div>

<style>
* {
  font-family: Arial, sans-serif;
}

.custom-grid-stacked {
  display: flex;
  flex-direction: column;
  gap: 30px;
  align-items: center;
}

.square-row {
  display: flex;
  gap: 40px;
  justify-content: center;
}

.square-row.center {
  justify-content: center;
}

.rectangle {
  width: 360px;
  height: 200px;
  border-radius: 12px;
  box-shadow: 2px 2px 6px rgba(0,0,0,0.1);
  padding: 20px;
  cursor: pointer;
  text-align: center;
  position: relative;
  transition: background-color 0.3s ease;
  display: flex;
  align-items: center;
  justify-content: center;
  flex-direction: column;
  overflow: hidden;
  color: #000;
}

.rect1 {
  background-color: #cad2c5;
}

.rect2 {
  background-color: #84a98c;
}

.rect3 {
  background-color: #52796f;
  color: #fff;
}

.rectangle:hover {
  filter: brightness(1.05);
}

.rectangle .title {
  font-size: 1.2em;
  font-weight: bold;
  transition: opacity 0.3s ease;
}

.rectangle .content {
  display: none;
  font-size: 0.65em;
  font-weight: normal;
  text-align: left;
  line-height: 1.25em;
}

.rectangle.active .title {
  display: none;
}

.rectangle.active .content {
  display: block;
}
</style>

<script>
function toggleText(el) {
  el.classList.toggle('active');
}
</script>


```


------------------------------------------------------------------------



# Small Area Estimation Overview

## Small Area Estimation Overview

> Small Area Estimation (SAE) is a statistical methodology that allows us to **improve the precision** of our estimates when we work with survey data for which **sample sizes** are limited or even zero.


::: notes
-   The reason we sometimes we don't have data is because these surveys are designed to aggregate the data at national or regional levle, but not for small areas (like neighbourhoods).

-   In urban Analitics we are very interested in this more granular estimates, rather than national or regional estimates.
:::

------------------------------------------------------------------------

## Small Area Estimation Overview

- Why are sample sizes **too small**?
  - Surveys are designed for specific levels of aggregation.
  - For instance, the NTS is designed for national-level and regional-level aggregation.
  - If we want to aggregate at a higher spatial resolution, sample sizes might be too small or even zero.


------------------------------------------------------------------------

## Small Area Estimation Overview

```{r, message=FALSE, echo=FALSE}
# Load library
library(emdi)
library(sf)
library(ggplot2)
library(purrr)
library(dplyr)

# Load spatial data
load_shapeaustria()
shape_austria_dis <- st_set_crs(shape_austria_dis, 4326) # Set CRS

# Create random points in the map
set.seed(123)
n_polygons <- nrow(shape_austria_dis)
shape_austria_dis$n_points <- sample(0:100, n_polygons, replace = TRUE)
sample_points <- function(polygon, n) {
  if (n == 0) return(NULL)
  st_sample(polygon, size = n, type = "random")
}
points_list <- map2(shape_austria_dis$geometry, shape_austria_dis$n_points, sample_points)
points_sf <- points_list %>%
  compact() %>%                     # Remove NULLs (i.e. 0-point areas)
  do.call(c, .) %>%                 # Combine into one geometry list
  st_as_sf()                        # Convert to sf object

points_sf$poly_id <- rep(shape_austria_dis$ID[shape_austria_dis$n_points > 0],
                         shape_austria_dis$n_points[shape_austria_dis$n_points > 0])
points_sf <- points_sf %>% st_set_crs(4326)


points_sf$observation <- "Sampled observation"

austria <- st_union(shape_austria_dis)

ggplot() +
  geom_sf(data = austria, col = "black") +
  geom_sf(data = shape_austria_dis, fill = "white", color = NA) +
  geom_sf(data = points_sf, aes(color = observation), size = 0.5, alpha = 0.4, show.legend = TRUE) +
  scale_color_manual(values = "black", name = "") +
  labs(
          title = "Sampled observations per district",
  ) +
  theme_void() +
  guides(color = guide_legend(override.aes = list(size = 2, alpha = 1)))  # Make legend point more 

```

::: notes
-   Survey data is usually designed to be used at certain aggregation levels.
-   Therefore, the way samples are collected are meant to support the level of aggregation in the survey design.
:::

------------------------------------------------------------------------

## Small Area Estimation Overview

```{r, message=FALSE, echo=FALSE}
# Load library
library(emdi)
library(sf)
library(ggplot2)
library(purrr)
library(dplyr)

# Load spatial data
load_shapeaustria()
shape_austria_dis <- st_set_crs(shape_austria_dis, 4326) # Set CRS

# Create random points in the map
set.seed(123)
n_polygons <- nrow(shape_austria_dis)
shape_austria_dis$n_points <- sample(0:100, n_polygons, replace = TRUE)
sample_points <- function(polygon, n) {
  if (n == 0) return(NULL)
  st_sample(polygon, size = n, type = "random")
}
points_list <- map2(shape_austria_dis$geometry, shape_austria_dis$n_points, sample_points)
points_sf <- points_list %>%
  compact() %>%                     # Remove NULLs (i.e. 0-point areas)
  do.call(c, .) %>%                 # Combine into one geometry list
  st_as_sf()                        # Convert to sf object

points_sf$poly_id <- rep(shape_austria_dis$ID[shape_austria_dis$n_points > 0],
                         shape_austria_dis$n_points[shape_austria_dis$n_points > 0])
points_sf <- points_sf %>% st_set_crs(4326)



points_sf$observation <- "Sampled observation"

ggplot() +
  geom_sf(data = shape_austria_dis, fill = NA, color = "black") +
  geom_sf(data = points_sf, aes(color = observation), size = 0.5, alpha = 0.4, show.legend = TRUE) +
  scale_color_manual(values = "black", name = "") +
  labs(
      title = "Sampled observations per district",
  ) +
  theme_void() +
  guides(color = guide_legend(override.aes = list(size = 2, alpha = 1)))  # Make legend point more 

```

::: notes
-   This can be problematic when we want to aggregate the data at a level that is lower than the survey recommended aggregation level.
-   For instance, the NTS is not designed to produce robust data below regional level.
-   If we want to aggregate at a higher spatial resolution than recommended, we might end up with areas or domains for which sampled observations are very small or even zero.
:::

---

## Small Area Estimation Overview

```{r}
# Load geospatial data and set CRS
load_shapeaustria()
shape_austria_dis <- st_set_crs(shape_austria_dis, 4326) # Set CRS

# Calculate the number of observations per district in the sample
smp_district_summary <- eusilcA_smp %>%
  group_by(district) %>%
  summarise(
    n = n(),  # Count of observations
    across(where(is.numeric), mean, na.rm = TRUE)
  ) %>%
  ungroup()

# Join the summarised sample to the district boundary data and leave NAs
shape_district_summary <- shape_austria_dis %>%
  left_join(smp_district_summary, by = c("PB" = "district"))

# Map
ggplot() +
  geom_sf(data = shape_district_summary, aes(fill = n), col = NA) +
  scale_fill_viridis_c(option = "D", direction = 1) +
  labs(
    title = "Number of Observations per District",
    fill = "Sample Size"
  ) +
  theme_void()
```

::: notes
-   In the example we will work with during the practical, we will use a dummy survey data sample for which some areas have no observations.
-   We will see how this can be problematic when using certain small area estimation methods.
:::

------------------------------------------------------------------------

## Small Area Estimation Overview

> SAE methods help us solve this issue by using [**models**]{.underline} that include additional information ([**auxiliary data**]{.underline}) in the form of covariates. This models better explain the variation in our data, and produce more [**reliable estimates**]{.underline}.

# SAE methods

## SAE methods overview

::: incremental
-   Today we will talk about the three main small area estimation methodologies: **direct estimators, area-level models, and unit-level models**.

-   We can think of them as increasingly complex methods that will help us improving our estimates.

-   As complexity increases, our estimates become more accurate. However, this also means that the data and computational requirements of the models also increase.
:::

## SAE methods overview

![](Figures/simple_diagram_methods.png){fig-align="center" width="2000"}

::: notes
-   When we talk about precision we talk about the reliability or stability of an estimate for a small area. It reflects how much uncertainty we or variability is associated with that estimate.

-   You can think of it as a confidence interval -- as your model's precision improves, your confidence interval narrows.
:::

--- 

## SAE methods overview

![](Figures/SAE_methods.jpg){fig-align="center"}

::: notes
-   The methods we will learn about today are part of a wider family of SAE methods. However, we will focus on these three for the sake of time.
:::

--- 

## Our example

::: incremental

- Example: Estimating income for Austrian districts.
- We are interested in calculating the income levels of the 94 Austrian districts using SAE methods.
- We have a survey with individual records at District level. 
- For some areas we have very limited observations and for others we have no observations at all.
:::

---

## Our example

```{r}
# Load geospatial data and set CRS
load_shapeaustria()
shape_austria_dis <- st_set_crs(shape_austria_dis, 4326) # Set CRS

# Calculate the number of observations per district in the sample
smp_district_summary <- eusilcA_smp %>%
  group_by(district) %>%
  summarise(
    n = n(),  # Count of observations
    across(where(is.numeric), mean, na.rm = TRUE)
  ) %>%
  ungroup()

# Join the summarised sample to the district boundary data and leave NAs
shape_district_summary <- shape_austria_dis %>%
  left_join(smp_district_summary, by = c("PB" = "district"))

# Map
ggplot() +
  geom_sf(data = shape_district_summary, aes(fill = n), col = NA) +
  scale_fill_viridis_c(option = "D", direction = 1) +
  labs(
    title = "Number of Observations per District",
    fill = "Sample Size"
  ) +
  theme_void()
```


# Direct estimation methods

## Direct estimation methods

:::: {.incremental style="font-size: 90%;"}
-   This is the simplest approach to small area estimation.
-   Intuitively, the direct estimator calculates the mean of the survey variable of interest for each area.
- *Example: We take the individual incomes and calculate the average for each district.*
:::

## Direct estimation methods

-   In SAE, this mathematical formulation is known as the **Horvitz-Thompson** estimator.

::: {.fragment .fade-in style="font-size: 90%;"}
$$
\hat{\bar{Y}_d} = \frac{1}{N_d} \sum_{i \in s_d} {w_{di}} Y_{di}
$$

where $\hat{\bar{Y}_d}$ is our estimate for area $d$; $N_d$ is the total population of domain $d$, $Y_{di}$ is the observed value for individual $i$ in domain $d$; and $w_{di}$ is the **sampling weight** of observation $i$ in domain $d$.
:::

------------------------------------------------------------------------

## A quick note on survey sampling

::: {.incremental style="font-size: 80%;"}
-   When a survey is run, the survey design ensures that it is as representative as possible of the full population --i.e. all groups in the population are included in the survey, in a proportion that follows that of the total population.

-   There is usually documentation accompanying the survey data explaining the details on how the survey sampling was done.

-   As a result, survey data often comes with **sampling weights**, which represent the probability of each observation included in the sample.

-   Additionally, the survey can contain more sophisticated attributes, that account for different subgroups within the population --this is known as **stratification**-- or account for sampling with or without replacement.
:::

------------------------------------------------------------------------

## A quick note on survey sampling

![](Figures/sampling.png)

------------------------------------------------------------------------

## Direct estimation methods

- **What happens if we have zero sample sizes?** The direct estimator is a *fancy* mean. Therefore, if there are no observations for an area, we cannot estimate the value of our target variable.

![](Figures/direct_results.png){fig-align="center"}

---

## Direct estimation methods

::: incremental

- Additionally, for areas with small sample sizes, our estimates might be unreliable.
- What does that mean?
  - Example: You want to estimate the mean income of a region, and you randomly sample 2 individuals to draw your estimates (very small sample!). Turns out that those two individuals are the richest ones in the area -- You might think that the area is very wealthy, but you just sampled the outliers! **Your estimates will be biased.**

- How do you solve this problem? You can use more sophisticated methods **(model-based SAE methods)** which incorporate additional information allowing you to make *better informed* guesses about your target variable.

:::
---

## Direct methods: Advantages and limitations

::: {.incremental style="font-size: 80%;"}
### Advantages

-   The direct estimation method works well when the sample has good coverage across all areas you are interested in.
-   It does not rely on external data, as it only uses observed data from the survey.
-   It is intuitive, and easy to interpret.

### Limitations

-   Unreliable for areas where you have small samples -- estimates can be very "noisy" (it is not very precise).
-   Not available for an area of interest for which we have no respondents.
:::

---


# Model-based estimation methods

## Model-based estimation methods

::: {.incremental style="font-size: 100%;"}
-   Direct methods are only reliable under large enough samples. However, this is not usually the case when we work with real survey data and small areas.

-   Furthermore, we usually have access to additional sources of information, not just survey data --e.g. census data, administrative data-- which can tell us more about the target variable.

-   We can therefore take advantage of this extra information and improve our estimates by **"borrowing strength"** from these other sources.

-   **Model-based methods** allow us to incorporate **auxiliary information** to improve our estimates.
:::

---

## 

------------------------------------------------------------------------

## Model-based estimation methods

::: incremental

- Imagine you have information about number of vehicles per household at district level, and you want to estimate the district's income levels.
- Number of vehicles per household is a good indicator of income levels.
- Model-based methods allow you to incorporate this *auxiliary information* to your estimator in order to improve your predictions on income levels.

:::


## Model-based estimation methods

::: {.incremental style="font-size: 90%;"}
-   The type of data we incorporate into our analysis will determine the type of model-based estimation method we will use.

-   Model-based methods are divided into **area-level models**, and **unit-level models**:

    -   **Area-level models** use aggregated auxiliary data at domain level (e.g. average house prices, age distribution)
    -   **Unit-level models** also include individual (unit)-level data from surveys (e.g. paid rent, individual age).
:::

# Area-level models

## Area-level models

::: {.incremental style="font-size: 80%;"}
-   Area-level models only use covariates at domain level.

-   Formally, they are estimated in two steps using what is known as the **Fay-Herriot model**:
:::

::: {.fragment style="font-size: 80%;"}
-   First, the we calculate the direct estimator (the same one we learnt about earlier):

    $$
    \hat{\theta}_d^{DIR} = \theta_d + \epsilon_d
    $$

where $\hat{\theta}_d^{DIR}$ is the direct estimate for area $d$,formed by the true value $\theta_d$ plus the sampling error $\epsilon_d$, which reflects the sampling error.
:::

---

## Area-level models

::: incremental

- The term $\hat{\theta}_d$ indicates that we are making an *estimate* of our variable of interest based on a sample for each area *d*.
- The term $\epsilon_d$ represents the sampling error, or the difference between your estimate and the *true value* of your variable. The error term indicates that our estimate is not perfect, and deviates from the real value because of random chance derived, for instance, from our sampling method.

:::

------------------------------------------------------------------------

## Area-level models

::: {.fragment style="font-size: 80%;"}
-   Next, the true values of the variable $\theta_d$ are modelled using area-level covariates.
- This is the so-called *sampling model* in the Fay-Herriot model:

    $$
    \theta_d = \mathbf{x}_d^T \boldsymbol{\beta} + u_d, \quad u_d \sim_{\text{ind}} N(0, \sigma_u^2), \quad d = 1, \ldots, M
    $$

where $\mathbf{x}_d^T$ are the covariates for area $d$, with their respective $\beta$ coefficients (the fixed effects), and $u_d$ is the area-level random effect, capturing the unexplained differences between areas.
:::

---

## Area-level models

::: {.incremental style="font-size: 75%;"}

- In this part of the model is where we *borrow strength* from auxiliary information to improve the estimates of our small areas. The $\mathbf{x}_d^T \boldsymbol{\beta}$ part of the equation incorporates the external information at area level ($d$) and estimates its effect ($\beta$) on the final estimate.

- $u_d$ represents the part of the true value ($\theta_i$) that cannot be explained by the auxiliary variables and that is unique to each domain. For instance, in Vienna, it might represent the "capital city" effect.

- The model assumes that this area-level random effects are independent and normally distributed (IID). This means that, on average, the deviation from each area's true value is similar for all the domains, and that the income levels in one area are unrelated to income levels in neighbouring areas.

:::

------------------------------------------------------------------------

## Spatial area-level models

::: {.incremental style="font-size: 75%;"}

- In reality, we know that this is usually not true: neighbouring areas tend to look alike (Tobler's First Law of Geography).

-   We can therefore include space in our models by adding a spatial effect. This will ensure that our model understands that areas that are closer will have estimates that are more similar than those of areas further away.

::: {.fragment style="font-size: 100%;"}

$$
u = \rho_1 Wu + \epsilon
$$

where $W$ is the adjacency matrix which tells us which districts are neighbours; $\rho_1$ is the spatial autorregression parameter, a single value between -1 and 1 that tells us how strong the relationship between neighbours is; $u_d$ is the area of interest; and $\epsilon$ is the error term.

:::
:::

---

## Area-level models -- The intercept-only model

::: {.incremental style="font-size: 75%;"}

- What happens if we do not have access to area-level auxiliary information?
- Do not worry! We can still use area-level modelling.
- In this case, we will use the intercept-only model, which allows us to generate estimates based on a combination of the direct estimates for each domain, and the global average.
- The weight of each component will depend on the variance of the sampling error: 
  - For areas with large samples, the direct estimator will have more weight than the global average.
  - For areas with small samples, the global average will have more weight.
  - For out-of-sample areas, the global mean will be assigned as the *best guess*
:::

---

## Area-level models -- The intercept-only model

- In this case, the equation of the intercept-only model is:

$$
    \theta_d = \beta_0 + u_d, \quad u_d \sim_{\text{ind}} N(0, \sigma_u^2), \quad d = 1, \ldots, M
$$

where $\beta_0$ is the global average, and $u_d$ represents the area-level random effects.

---

## Area-level vs Direct estimator

::: {.incremental style="font-size: 90%;"}

- The area-level model can calculate estimates for out-of-sample domains, although their accuracy will depend on how good the model is at explaining uncertainty through the auxiliary variables.

- In general, when comparing the estimates of the direct estimator and area-level model, we observe how the uncertainty around our estimates decreases with the latter method.

- We will further explore this in the practical.
:::

---

## Area-level vs Direct estimator

![](Figures/fay-herriot_estimates.png){fig-align="center"}


---

## Area-level vs Direct estimator

:::::: columns
:::: {.column width="49%"}
![](Figures/direct_estimates_uncertainty.png)
:::


::: {.column width="49%"}
![](Figures/area_level_estimates_uncertainty.png)
:::
::::::

------------------------------------------------------------------------

## Area-level models: Advantages and limitations

::: {.incremental style="font-size: 100%;"}
-   The main advantage of using an area-level model is that we increase the precision of our estimates by adding auxiliary information at area level.
-   Additionally, this allows us to obtain predictions for out-of-sample domains, even when no auxiliary information is available.
-   However, these models require access to area-level data, and are not as straightforward to interpret as direct methods.
:::

------------------------------------------------------------------------



# Unit-level models

## Unit-level models

::: {.incremental style="font-size: 65%;"}
-   Unit-level models are the most complex type of SAE method we will talk about today.

-   Unit-level models incorporate auxiliary data at individual (or household, or whichever unit of analysis we are using) level.

-   These models **assume that unit-specific auxiliary data** is available for each population element $i$ in each area $d$.

-   In general, they allow for producing more precise estimates than direct estimators and area-level models.

-   Similarly to area-level models, unit-level models also produce estimates for out-of-sample domains (assuming we have access to unit-level auxiliary data).
:::

## Unit-level models

-   The most basic unit-level models are known as **nested error** or **Battese, Harter, and Fuller** models.

- These type of models are called *nested* because they include both area-level and unit-level random effects:

::: {.fragment .incremental style="font-size: 65%;"}
$$
  y_{di} = \mathbf{x}_{di}^T \beta + u_d + \epsilon_{di} , \quad u_d \sim_{\text{ind}} N(0, \sigma_u^2), \quad \epsilon_{di} \sim_{\text{ind}} N(0, \sigma_\epsilon^2)
$$

-   where $y_{di}$ is the unit-level outcome for individual $i$ in area $d$,\

-   $\mathbf{x}_{di}^T \beta$ represents the individual-level covariates and their coefficients,\

-   $u_d$ is the area-level random effect, and\

-   $\epsilon_{di}$ is the unit-level error term, capturing individual variability within each area.

-   This data is then aggregated at any desired spatial resolution.
:::

---

## Unit-level models

::: {.incremental style="font-size: 75%;"}

- The area-level random effect accounts for those attributes of the domain that cannot be explained by the area-level auxiliary variables we include in the model, and contribute to variations in the target variable.

- E.g. Vienna being the capital, and this leading to higher average income levels.

- Similarly, the unit-level random effects account for individual-level characteristics that cannot be explained by the unit-level covariates included in the model.

- E.g. A person being the head a bank and earning a very high salary (assuming we do not include occupation as a covariate).
:::

## Unit-level models

::: {.incremental style="font-size: 70%;"}

- The unit-level model can be interpreted as a bottom-up approach to small area estimation.
- This gives us more flexibility, since the use of individual-level information allows us to aggregate the data at other domain levels without being constrained by the resolution of the area-level of the auxiliary covariates (as opposed to area-level models).
- The data requirements, however, are much higher for unit-level models: we need access to individual data, which can come from census microdata or synthetic populations. These might not always be available.

:::

## Unit-level models: Advantages and limitations

::: {.incremental style="font-size: 70%;"}
### Advantages

-   Unit-level models are the most precise out of the SAE methods we have learnt about today, assuming data is available.

-   Unit-level models constitue a "bottom-up" approach to SAE --they use the most granular available data (unit-level).

-   They are very flexible, since they allow for aggregation at any desired geographical level.

### Limitations

-   They require access to both unit-level covariates and population-level auxiliary data, which may not always be available.

-   Estimation is more complex and computationally demanding than area-level models, especially when working with large populations.
:::



------------------------------------------------------------------------

# Summary and key takeaways

## Summary and key takeaways

::: {.incremental style="font-size: 80%;"}
-   Small Area Estimation helps us produce reliable estimates in areas where survey data is sparse or missing.

-   We explored three SAE approaches:

    -   Direct estimators: straightforward, but unreliable for small samples.
    -   Area-level models: use domain-level covariates to improve estimates.
    -   Unit-level models: use individual-level data for even more precision.

-   Model-based methods "borrow strenght" from auxiliary information (e.g. census data, spatial data), allowing us to reduce noise and fill data gaps.

-   Spatial models can improve accuracy furhter by accounting for the spatial autocorrelation of the data.
:::

## QR code for practical

![https://cpeiretgarcia.github.io/SAE_workshop_CUPUM/](qr-code.png){fig-align="center"}

------------------------------------------------------------------------

## Additional Resources